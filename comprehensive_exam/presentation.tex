\documentclass[10pt, compress, notheorems, aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm, amsmath, mathrsfs, amsfonts, bm, mathtools}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{breqn}
\usepackage[backend=biber, sorting=none]{biblatex}
	\addbibresource{ref.bib}
\usepackage{tabulary}
\usepackage{multirow}
\usepackage{filecontents}
\usepackage{comment}

\usefonttheme{serif}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose \bgroup \originalleft}
\renewcommand{\right}{\aftergroup \egroup \originalright}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank} 

% \hypersetup{colorlinks=true} % Enable colored hyperlinks

\definecolor{utablue}{RGB}{0,100,177}% Official RGB code for UTA blue 
\definecolor{utaorange}{RGB}{245,128,38} 
\definecolor{utablue2}{RGB}{0,68,124}
\definecolor{utablue3}{RGB}{212,239,252}
\definecolor{utablue4}{RGB}{231,246,253}%R-231 G-246 B-253

\setbeamercolor{palette primary}{bg=utablue2,fg=utablue2}
\setbeamercolor{palette secondary}{bg=utablue2,fg=white} %footer color
\setbeamercolor{palette tertiary}{bg=black,fg=utablue2}% header color
\setbeamercolor{palette quaternary}{bg=utablue2,fg=black}
\setbeamercolor{structure}{fg=utablue} % itemize, enumerate, etc
\setbeamercolor{section in toc}{fg=utablue2} % TOC sections
\setbeamercolor{subsection in toc}{fg=utablue} 
\setbeamercolor{subsection in head/foot}{bg=utablue2,fg=utablue2}
\setbeamercolor{block title}{bg=utablue4,fg=black}
\setbeamercolor{title}{bg=utablue,fg=white}

\setbeamertemplate{enumerate items}[circle] % enumerates each item with a number inside a circle
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{section in toc}[circle]
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{frametitle}[default][left]
\setbeamertemplate{theorems}[numbered] % to number
\setbeamertemplate{navigation symbols}{\insertslidenavigationsymbol, }
\setbeamertemplate{background canvas}{
	\includegraphics[height = \paperheight, width = \paperwidth]{123_Page_04.png}
}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{title page}{
	\vspace{6em}
	\centering
	\begin{beamercolorbox}[sep = 10pt, center, rounded = false]{title}
		\usebeamerfont{title}
		\inserttitle  
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{author}
		\usebeamerfont{author}
		\insertauthor
	\end{beamercolorbox}
	\begin{beamercolorbox}[sep = 10pt, center]{institute}
		\usebeamerfont{institute}
		\insertinstitute
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{date}
		\usebeamerfont{date}
		\insertdate
	\end{beamercolorbox}
}

 \AtBeginSection[]{
 	{\setbeamertemplate{background canvas}{\includegraphics[height=\paperheight,width=\paperwidth]{123_Page_02.png}}
	 	\begin{frame}<beamer>
	    	\frametitle{Section \thesection}
	    	\tableofcontents[currentsection]
	    \end{frame}
    }
}

\begin{document}

\title{Bayesian Hierarchical Dynamic Factor Models}
     
\author{
	Anthony M. Thomas, Jr.\\
	\footnotesize
	\href{mailto:anthony.thomas3@mavs.uta.edu}{anthony.thomas3@mavs.uta.edu}
}

\institute{
	Department of Mathematics\\ 
	The University of Texas at Arlington
}

\date{\today}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{UTA-title-page.png}}
\begin{frame}[plain]
	\titlepage
\end{frame}
}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{123_Page_02.png}}
\begin{frame}[plain]
\frametitle{Contents}
\tableofcontents
\end{frame}
}

\section{Background}
\subsection{Bayesian Inference}
\begin{frame}
	\frametitle{Bayesian Inference}
	\textbf{Bayesian Inference} can be described by two parts:
	\begin{enumerate}
		\item Build a model based on data $\bm X$ and parameters $\bm \Theta = \left\{ {\bm \Theta}_1, {\bm \Theta}_2 \right\}$
			\begin{itemize}
				\item[--] Likelihood: $p \left( {\bm X} | {\bm \Theta} \right)$
				\item[--] Prior: $p \left( {\bm \Theta}\right)$
			\end{itemize}
		\item Compute the posterior
			\begin{itemize}
				\item[--] Posterior: $$p \left( {\bm \Theta} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm \Theta} \right) p \left( {\bm \Theta} \right) }{ p \left( {\bm X} \right)}$$
				\item[--] Report summaries, e.g. posterior expectations
				$$\E \left[ h( {\bm \Theta} ) | {\bm X} \right]$$
				or marginal posterior expectations
				$$\E \left[ h( {\bm \Theta}_i ) | {\bm X} \right]$$	
		\end{itemize}
	\end{enumerate}
\end{frame}

\subsection{Factor Analysis}

\begin{frame}
	\frametitle{Factor Analysis}
	\begin{itemize}
		\item \textbf{Factor Analysis} (FA) is a method that assumes that the covariance structure of a set of cross-sectional observations can be described in terms of a linear combination of latent variables called factors
		\item A sample of $P$ observations are related to a set of factors through the equation
			\begin{equation}
				{\bm X}_i = {\bm \Lambda} {\bm F}_i + {\bm e}_i, \qquad i = 1, \ldots, P
			\end{equation}
			where 
			\begin{itemize}
				\item[--] ${\bm X}_i = \left(X_{1i}, \ldots, X_{Ni} \right)^{\top}$ denotes a vector of observations for variable $i$
				\item[--] ${\bm F}_i = \left(F_{1i}, \ldots, F_{Ki} \right)^{\top}$ denotes a vector of factors for variable $i$
				\item[--] ${\bm e}_i = \left(e_{1i}, \ldots, e_{Ni} \right)^{\top}$ denotes a vector of measurement errors and idiosyncratic (unique) factors for variable $i$
				\item[--] ${\bm \Lambda} = \left[ \lambda_{nk} \right]_{N \times K}$ denotes a matrix of factor loadings
			\end{itemize}
	\end{itemize}
\end{frame}
\begin{frame}
	\begin{itemize}
		\item The following assumptions are typically made in FA:			
			\begin{enumerate}
				\item $\rank \left( {\bm \Lambda} \right) = K$ 
				\item $\E \left[ {\bm X}_i \right] = \E \left[ {\bm e_i} \right] ={\bf 0}_N$ and $\E \left[ {\bm F_i} \right] ={\bf 0}_K \quad \forall i$
				\item $\Var \left( {\bm F}_i \right) = {\bf I}_K$ and $\Var \left( {\bm e}_i \right) = {\bm \Sigma}$ where ${\bm \Sigma} = \text{diag} \left( \sigma_1^2, \ldots, \sigma_N^2 \right) \quad \forall i$
				\item $\Cov \left( {\bm F}_i, {\bm e}_i \right) = {\bf 0}_{K \times N} \quad \forall i$
			\end{enumerate}
		\item Under these assumptions it follows that 
			\begin{equation*}
				\Var \left( {\bm X}_i \right)= {\bm \Lambda} {\bm \Lambda}^{\top} + {\bm \Sigma} \quad \forall i
			\end{equation*}
		\item Typical uses of FA:
			\begin{enumerate}
				\item Dimension reduction: explain the covariation between $N$ variables using $K < N$ factors
				\item Data interpretation: find factors that explain the covariation
				\item Theory testing: test whether a hypothesized factor structure fits observed data
			\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Time Series Factor Analysis}
	\begin{itemize}
		\item In \cite{gilbert2005time} FA is extended to time series data as \textbf{time series factor analysis} (TSFA)
		\item A sample of $T$ time series observations are related to the factors through the equation
			\begin{equation}
				{\bm X}_t = {\bm \Lambda} {\bm F}_t + {\bm e}_t \qquad t = 1, \ldots, T
			\end{equation}
			where
			\begin{itemize}
				\item[--] ${\bm X}_t = \left( X_{1t}, \ldots, X_{Nt} \right)^{\top}$ denotes a vector of observations at time $t$
				\item[--] ${\bm F}_t = \left( F_{1t}, \ldots, F_{K_F t} \right)^{\top}$ denotes a vector of factors at time $t$
				\item[--] ${\bm e}_t$ is a vector of measurement errors and idiosyncratic factors at time $t$
				\item[--] ${\bm \Lambda} = \left[ \lambda_{nk} \right]_{N \times K}$ denotes a matrix of factor loadings
			\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Dynamic Factor Analysis}
\begin{frame}
	\frametitle{Dynamic Factor Analysis}
	\begin{itemize}
		\item In \textbf{dynamic factor analysis} (DFA) the factors are assumed to not only affect the observations contemporaneously, but affect them through their lags as well:
			\begin{equation}
				X_{nt} = {\lambda}^n \left( L \right) {\bm F}_t + {\bm e}_t \quad n = 1, \ldots, N
			\end{equation}
			where 
			\begin{equation*}
				{\lambda}^n \left( L \right) = {\lambda}^n_0 + {\lambda}^n_1 L + \cdots + {\lambda}^n_q L^q
			\end{equation*}
			\begin{equation*}
				L^s {\bm F}_t = {\bm F}_{t-s} \quad \forall s \geq 0
			\end{equation*}
			is a distributed lag polynomial of factor loadings in the lag operator $L$ for the $n$th series
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Dynamic Factor Analysis}
	\begin{itemize}
		\item In DFA the factors are modeled as a time series process
		\item The time series process is commonly taken to be a vector autoregressive process, i.e.
			\begin{equation}
				{\bm \Psi} \left( L \right) {\bm F}_t = {\bm \varepsilon}_t
			\end{equation}
			where 
			\begin{equation*}
				{\bm \Psi} \left( L \right) = {\bf I}_K - {\bm \Psi}_1 L - \cdots  - {\bm \Psi}_p L^p
			\end{equation*}
			is a matrix polynomial of autocorrelation coefficients in the lag operator $L$
	\end{itemize}
\end{frame}

\section{Hierarchical Dynamic Factor Analysis}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Analysis}
	\begin{itemize}
		\item Suppose you organize a large panel of data into $B$ blocks (e.g. Production, Employment, Demand, etc.) and each block $b$ has $N_b$ series
		\item $N = \sum_{b=1}^B N_b$
		\item A block may be divided into subblocks (e.g. Demand: Retail Sales, Auto Sales)
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item In \cite{moenchDynamicHierarchicalFactor2013}, the authors generalize the dynamic factor model by positing that for each $t$, the $n$th series in a given block $b$, denoted by $ X_{bnt}$, has three sources of variation:
			\begin{enumerate}
				\item idiosyncratic
				\item block-specific 
				\item common
			\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item A three-level representation of the data for $b = 1, \ldots, B$ and $n = 1, \ldots, N_b$ is given as 
			\begin{align}
				X_{bnt} &= {\bm \lambda}^n_{G.b} \left( L \right) {\bm G}_{bt} + e_{Xbnt} \\
				{\bm G}_{bt} &= {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_t + {\bm e}_{Gbt} \\
				{\bm \Psi}_F \left( L \right) {\bm F}_t &= {\bm \epsilon}_{Ft},
			\end{align}
			where
			\begin{itemize}
				\item[--] ${\bm \lambda}^n_{G.b} \left( L \right)$ denotes a distributed lag polynomial of block-level factor loadings
				\item[--] ${\bm \Lambda}_{F.b} \left( L \right)$ denotes a distributed lag matrix polynomial of common factor loadings
				\item[--] ${\bm G}_{bt} = \left( G_{b1t}, \ldots, G_{bK_{Gb}t} \right)^{\top}$ denotes the block-level factors
				\item[--] ${\bm F}_t = \left( F_{1t}, \ldots, F_{K_F t} \right)^{\top}$ denotes the common factors
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item For some blocks, it may be appropriate to break up the data into subblocks, which adds another source of variation
		\item Let $Z_{bsnt}$ be the $n$th series in subblock $s$ of block $b$
		\item A four-level representation of the subblock data is given as
			\begin{align}
				Z_{bsnt} &= {\bm \lambda}^n_{H.bs} \left( L \right) {\bm H}_{bst} + e_{Zbsnt} \\
				{\bm H}_{bst} &= {\bm \Lambda}_{G.bs} \left( L \right) {\bm G}_{bt} + {\bm e}_{Hbst} \\
				{\bm G}_{bt} &= {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_t + {\bm e}_{Gbt} \\
				{\bm \Psi}_F \left( L \right) {\bm F}_t &= {\bm \epsilon}_{Ft}
			\end{align}	
			where
			\begin{itemize}
				\item[--] ${\bm \lambda}^n_{H.bs} \left( L \right)$ denotes a distributed lag polynomial of subblock-level factor loadings
				\item[--] ${\bm \Lambda}_{G.bs} \left( L \right)$ denotes a distributed lag matrix polynomial of block-level factor loadings
				\item[--] ${\bm H}_{bst} = \left( H_{bs1t}, \ldots, H_{bsK_{Hbs}t} \right)^{\top}$ denotes the subblock-level factors
			\end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item The idiosyncratic components, the subblock-specific, block-specific, and common factors are assumed to be stationary, Gaussian autoregressive processes of orders $q_{Zbsn}$, $q_{Xbn}$, $q_{Hbsi}$, $q_{Gbj}$, and $q_{Fk}$, respectively, i.e.
			\begin{alignat*}{3}
				\psi_{Z.bsn} \left( L \right) e_{Zbsnt} &= \epsilon_{Zbsnt}, \quad & \epsilon_{Zbsnt} &\sim \mathcal{N} \left( 0, \sigma^2_{Zbsn} \right) \quad & n &= 1, \ldots, N_{bs} \\
				\psi_{X.bn} \left( L \right) e_{Xbnt} &= \epsilon_{Xbnt}, \quad & \epsilon_{Xbnt} &\sim \mathcal{N} \left( 0, \sigma^2_{Xbn} \right) \quad & n &= 1, \ldots, N_b \\
				\psi_{H.bsi} \left( L \right) e_{Hbsit} &= \epsilon_{Hbsit}, & \epsilon_{Hbsi} &\sim \mathcal{N} \left(0, \sigma^2_{Hbsi} \right) & i &= 1, \ldots, K_{Hbs} \\
				\psi_{G.bj} \left( L \right) e_{Gbjt} &= \epsilon_{Gbjt}, & \epsilon_{Gbjt} &\sim \mathcal{N} \left(0, \sigma^2_{Gbj} \right) & j &= 1, \ldots, K_{Gb} \\
				\psi_{F.k} \left( L \right) F_{kt} &= \epsilon_{Fkt}, & \epsilon_{Fkt} &\sim \mathcal{N} \left(0, \sigma^2_{Fk} \right) & k &= 1, \ldots, K_{F}
			\end{alignat*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item Not all series need to belong to blocks and subblocks
		\item In general, the data used in a four-level model are a mixture of $Z_{bsnt}$, $X_{bnt}$, and $X_{nt}$
	\end{itemize}	
\end{frame}

\subsection{State Space Representation}

\begin{frame}
	\frametitle{State Space Representation}
	\begin{itemize}
		\item Observed data (vector/matrix form):
			\begin{align*}
				{\bm Z}_{bst} &= {\bm \Lambda}_{H.bs} \left( L \right) {\bm H}_{bst} + {\bm e}_{Zbst} \\
				{\bm \Psi}_{Z.bs} (L) {\bm e}_{Zbst} &= {\bm \epsilon}_{Zbst}
			\end{align*}
			implies that the measurement equation is
			\begin{equation}
				\begin{aligned}
					{\bm \Psi}_{Z.bs} (L) {\bm Z}_{bst} &= {\bm \Psi}_{Z.bs} (L) {\bm \Lambda}_{H.bs} \left( L \right) {\bm H}_{bst} + {\bm \epsilon}_{Zbst} \\
					\tilde{\bm Z}_{bst} &= \tilde{\bm \Lambda}_{H.bs} \vec{\bm H}_{bst} + {\bm \epsilon}_{Zbst}, \quad {\bm \epsilon}_{Zbst} \sim \mathcal{N} \left( {\bf 0}, {\bm \Sigma}_{Zbs} \right)
				\end{aligned}
			\end{equation}
			with transition equation
			\begin{equation}
				\vec{\bm H}_{bst} = \vec{\bm \alpha}_{G.bst} + \tilde{\bm \Psi}_{Hbs} \vec{\bm H}_{bst-1} + \vec{\bm \epsilon}_{Hbst}, \quad \vec{\bm \epsilon}_{Hbst} \sim \mathcal{N} \left( {\bf 0}, {\bf 1} {\bf 1}^{\top} \otimes {\bm \Sigma}_{Hbs} \right)
			\end{equation}
			where ${\bm \alpha}_{G.bst} = {\bm \Psi}_{H.bs} \left( L \right) {\bm \Lambda}_{G.bs} \left( L \right) {\bm G}_{bt}$
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{State Space Representation}
	\begin{itemize}
		\item Subblock-level factors (vector/matrix form):
			\begin{align*}
				{\bm H}_{bt} &= {\bm \Lambda}_{G.b} \left( L \right) {\bm G}_{bt} + {\bm e}_{Hbt} \\
				{\bm \Psi}_{H.b} (L) {\bm e}_{Hbt} &= {\bm \epsilon}_{Hbt}
			\end{align*}
			implies that the (pseudo) measurement equation is
			\begin{equation}
				\begin{aligned}
					{\bm \Psi}_{H.b} (L) {\bm H}_{bt} &= {\bm \Psi}_{H.b} (L) {\bm \Lambda}_{G.b} \left( L \right) {\bm G}_{bt} + {\bm \epsilon}_{Hbt} \\
					\tilde{\bm H}_{bt} &= \tilde{\bm \Lambda}_{G.b} \vec{\bm G}_{bt} + {\bm \epsilon}_{Hbt}, \quad {\bm \epsilon}_{Hbt} \sim \mathcal{N} \left( {\bf 0}, {\bm \Sigma}_{Hb} \right)
				\end{aligned}
			\end{equation}
			with transition equation
			\begin{equation}
				\vec{\bm G}_{bt} = {\bm \alpha}_{F.bt} + \tilde{\bm \Psi}_{G.b} \vec{\bm G}_{bt-1} + \vec{\bm \epsilon}_{Gbt}, \quad \vec{\bm \epsilon}_{Gbt} \sim \mathcal{N} \left( {\bf 0}, {\bf 1} {\bf 1}^{\top} \otimes {\bm \Sigma}_{Gb} \right)
			\end{equation}
			where ${\bm \alpha}_{F.bt} = {\bm \Psi}_{G.b} \left( L \right) {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_{t}$
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{State Space Representation}
	\begin{itemize}
		\item Block-level factors (vector/matrix form):
			\begin{align*}
				{\bm G}_{t} &= {\bm \Lambda}_{F} \left( L \right) {\bm F}_{t} + {\bm e}_{Gt} \\
				{\bm \Psi}_{G} (L) {\bm e}_{Gt} &= {\bm \epsilon}_{Gt}
			\end{align*}
			implies that the (pseudo) measurement equation is
			\begin{equation}
				\begin{aligned}
					{\bm \Psi}_{G} (L) {\bm G}_{t} &= {\bm \Psi}_{G} (L) {\bm \Lambda}_{F} \left( L \right) {\bm F}_{t} + {\bm \epsilon}_{Gt} \\
					\tilde{\bm G}_{t} &= \tilde{\bm \Lambda}_{F} \vec{\bm F}_{t} + {\bm \epsilon}_{Gt}, \quad {\bm \epsilon}_{Gt} \sim \mathcal{N} \left( {\bf 0}, {\bm \Sigma}_{F} \right)
				\end{aligned}
			\end{equation}
			with transition equation
			\begin{equation}
				\vec{\bm F}_{t} = \tilde{\bm \Psi}_{F} \vec{\bm F}_{t-1} + \vec{\bm \epsilon}_{Ft}, \quad \vec{\bm \epsilon}_{Gbt} \sim \mathcal{N} \left( {\bf 0}, {\bf 1} {\bf 1}^{\top} \otimes {\bm \Sigma}_{Gb} \right)
			\end{equation}
	\end{itemize}
\end{frame}

\subsection{Data and Model Structure}
\begin{frame}
	\frametitle{Data and Model Structure}
	\begin{itemize}
		\item The authors organized a dataset consisting of $N=445$ series giving $T=227$ observations into 5 blocks	
	\end{itemize}
	\begin{table}[ht]
		\centering
		\caption{Block Structure}
		\includegraphics[scale=0.55]{data-structure.png}
	\end{table}
\end{frame}

\begin{frame}
	\frametitle{Data and Model Structure}
	\begin{itemize}
		\item The distributed lag (matrix) polynomials of factor loadings are assumed to be constant (order 0), i.e.
			\begin{align*}
				{\lambda}^n_{H.bs} (L) &= {\lambda}^n_{H.bs 0} \\
				{\bm \Lambda}_{G.bs} (L) &= {\bm \Lambda}_{G.bs 0} \\ 
				{\bm \Lambda}_{F.b} (L) &= {\bm \Lambda}_{F.b 0} \\  
			\end{align*}
			\vspace{-3em}
		\item The (matrix) polynomials of autocorrelation coefficients are assumed to be of order 1, i.e.
			\begin{align*}
				{\psi}_{Z.bsn} \left( L \right) &= 1 - {\psi}_{Z.bsn1} L \\
				{\psi}_{X.bn} \left( L \right) &= 1 - {\psi}_{X.bn1} L \\
				{\psi}_{H.bsi} \left( L \right) &= 1 - {\psi}_{H.bsi1} L \\
				{\psi}_{G.bj} \left( L \right) &= 1 - {\psi}_{G.bj1} L \\
				{\psi}_{F.k} \left( L \right) &= 1 - {\psi}_{F.k1} L
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Data and Model Structure}
	\begin{itemize}
		\item They estimate one common factor, i.e. $K_F = 1$, one common factor per block, i.e. $K_{Gb}=1 \quad \forall b$, and one or two factors per subblock, i.e. $K_{Hbs} = 1, 2$
		\item For the cases where $K_{Hbs} = 2$, the factor loading matrices are assumed to be lower triangular with 1's along the diagonal, i.e.
			\begin{equation*}
				{\bm \Lambda}_{H.bs} = 
				\begin{bmatrix}
					1 & 0 \\ \lambda_{{H.bs}_{2,1}} & 1 \\ \lambda_{{H.bs}_{3,1}} & \lambda_{{H.bs}_{3,2}} \\ \vdots & \vdots \\ \lambda_{{H.bs}_{N_{bs},1}} & \lambda_{{H.bs}_{N_{bs},2}}
				\end{bmatrix}
			\end{equation*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Parameter Priors}
	\begin{itemize}
		\item All free factor loadings and autocorrelation coefficients are assigned independent standard Gaussian priors, i.e. 
			\begin{align*}
				\lambda_{(\cdot)} &\sim \mathcal{N} \left(0, 1 \right)	\\
				\psi_{(\cdot)} &\sim \mathcal{N} \left(0, 1 \right)	
			\end{align*}

		\item All variance parameters are assigned independent scaled-inverse chi squared distributions with 4 degrees of freedom and a scale of 0.01, i.e.
			\begin{equation*}
				\sigma_{(\cdot)}^2 \sim \text{scale-inv-} \chi^2 \left( 4, 0.01^2 \right)
			\end{equation*}
	\end{itemize}
\end{frame}


\subsection{Gibbs Sampler}
\begin{frame}
	\frametitle{Gibbs Sampler}
	\begin{itemize}
		\item The authors implement a Gibbs sampling algorithm based on a degenerate linear Gaussian state-space representation of the model to obtain samples from the posterior and compute posterior means
		\item 50,000 draws are used as a burn-in, i.e. discarded
		\item 50,000 more draws are obtained while storing every 50th draw to obtain a posterior sample size of 1,000
		\item The main idea of the algorithm is presented next
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Gibbs Sampler}
	\begin{itemize}
		\item Let ${\bm \Lambda} = \left( {\bm \Lambda}_H, {\bm \Lambda}_G, {\bm \Lambda}_F \right)$, ${\bm \Psi} = \left( {\bm \Psi}_F, {\bm \Psi}_G, {\bm \Psi}_H, {\bm \Psi}_Z \right)$, and ${\bm \Sigma} = \left( {\bm \Sigma}_F, {\bm \Sigma}_G, {\bm \Sigma}_H, {\bm \Sigma}_Z \right)$
	\end{itemize}
	\begin{enumerate}
		\item Organize the data into blocks and subblocks to get $Z_{bst}$ for $b = 1, \ldots, B, s = 1, \ldots, B_S$. Get initial values for $\left\{ H_{bst} \right\}$, $\left\{ G_{bt} \right\}$,  and $\left\{ F_t \right\}$ using principal components. Use these to produce initial values for ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}$
		\item Conditional on ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}, \left\{ G_{bt} \right\}, \left\{ F_t \right\}$ and the data, draw $\left\{ H_{bst} \right\} \quad \forall b, s$
		\item Conditional on ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}, \left\{ H_{bst} \right\}, \left\{ F_t \right\}$ and the data, draw $\left\{ G_{bt} \right\} \quad \forall b$
		\item Conditional on ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}, \left\{ H_{bst} \right\}, \left\{ G_{bt} \right\}$ and the data, draw $\left\{ F_t \right\}$
		\item Conditional on $\left\{ H_{bst} \right\}$, $\left\{ G_{bt} \right\}, \left\{ F_t \right\}$ and the data, draw ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}$
		\item Return to 2
	\end{enumerate}
\end{frame}

\section{Variational Bayesian Inference}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	\begin{itemize}
		\item Consider a model with data ${\bm X}$ and latent variables ${\bm Z}$ (model parameters can be included)
		\item The goal is to compute the joint posterior of the latent variables given the data
	  		\begin{equation} \label{posterior}
	  			p \left( {\bm Z} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm Z} \right) p \left( {\bm Z} \right) }{ p \left( {\bm X} \right) }
	  		\end{equation}
 		  	\begin{itemize}
	  			\item[--] Likelihood: $p \left( {\bm X} | {\bm Z} \right)$
				\item[--] Prior: $p \left( {\bm Z}\right)$
				\item[--] Evidence: $p \left( {\bm X} \right)$
					\begin{equation} 			
						p \left( {\bm X} \right) = \int p \left( {\bm X}, {\bm Z} \right) \, d{\bm Z} \label{marginal.evidence}
					\end{equation}
			\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	\begin{itemize}
		\item For complex models (\ref{posterior}) and (\ref{marginal.evidence}) either have no closed-form or require high-dimensional integration which causes the ``inference problem"
		\item As a result, the joint posterior has to be approximated
		\item Markov chain Monte Carlo (MCMC) methods have been the gold standard to solve this problem
			\begin{enumerate}
				\item Construct an ergodic Markov chain on $\bm Z$ whose stationary distribution is the joint posterior $p \left( {\bm Z} | {\bm X} \right)$
				\item Sample from the chain to collect samples from the stationary distribution
				\item Approximate the posterior with an empirical estimate constructed from a subset of the collected samples
				\item Use the subset of collected samples to estimate expectations of interest
			\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	\begin{itemize}
		\item MCMC uses sampling to solve the inference problem
		\item MCMC methods eventually produce accurate results, but are usually computationally intensive for interesting models
		\item Variational Bayes (VB) uses optimization to solve the inference problem
		\item VB typical obtains results much faster
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	\begin{itemize}
		\item The main idea of the VB framework is to
			\begin{enumerate}
				\item Posit a family of ``nice" approximate densities $\mathcal Q$
				\item Find a member of that family that is ``closest" to the exact posterior
					\begin{equation}
						q^{\star} \left( {\bm Z} \right) = \argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) \label{objective}
					\end{equation}
					where 
					\begin{equation}
						\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm Z} | {\bm X} \right) \right]
					\end{equation}
			\end{enumerate}	
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	\begin{itemize}
		\item It follows that 
			\begin{equation*}
				\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] + \log p \left( {\bm X} \right)
			\end{equation*}
			which reveals that the objective in (\ref{objective}) depends on the evidence, thus it cannot be computed directly
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Evidence Lower Bound}
	\begin{itemize}
		\item The reason why $\text{KL} \left( q || p \right)$ is a desirable measure of ``closeness" is because it leads to a lower bound on $\log p \left( {\bm X} \right)$ called the \textbf{evidence lower bound} (ELBO)
			\begin{align*}
				\log p \left( {\bm X} \right) 
				&= \log \int q \left( {\bm Z} \right) \frac{p \left( {\bm X}, {\bm Z} \right)}{q \left( {\bm Z} \right)} \, d {\bm Z} \\
				&= \log \left( \E_q \left[ \frac{p \left( {\bm X}, {\bm Z} \right)}{q \left( {\bm Z} \right)} \right] \right) \\
				&\geq \E_q \left[ \log \frac{p \left( {\bm X}, {\bm Z} \right)}{q \left( {\bm Z} \right)} \right], \quad \text{ by Jensen's Inequality} \\
				&= \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] - \E_q \left[ \log q \left( {\bm Z} \right) \right] \\
				&= \text{ELBO} \left( q \right)
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Evidence Lower Bound}
	\begin{itemize}
		\item It follows that 
			\begin{equation*}
				\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \log p \left( {\bm X} \right) - \text{ELBO} \left( q \right)
			\end{equation*}
		\item Since $\log p \left( {\bm X} \right)$ is constant wrt $q \left( {\bm Z} \right)$ we can redefine the objective function as
			\begin{equation*}
				\argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) 
			\end{equation*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	\begin{itemize}
		\item The VB framework is now
			\begin{enumerate}
				\item Posit a family of ``nice" approximate densities $\mathcal Q$
				\item Find a member of that family that is ``closest" to the exact posterior, i.e.
				\begin{equation}
					q^{\star} \left( {\bm Z} \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) \label{new.objective}
				\end{equation}
			\end{enumerate}
		\item Solving this optimization problem is still difficult in general
		\item Using the mean-field assumption can make it easier
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Mean-Field Assumption}
	\begin{itemize}
		\item The mean-field assumption says to:
			\begin{enumerate}
				\item Partition the latent variables into $M$ groups, say ${\bm Z}_1, \ldots, {\bm Z}_M$
				\item Assume that the distributions in $\mathcal Q$ factorize across the groups, i.e.
				$$\mathcal Q = \left\{ q : q \left( {\bm Z} \right) = \prod_{m = 1}^M q_m \left( {\bm Z}_m \right) \right\}$$
			\end{enumerate}
		\item Learning the optimal $q$ now reduces to learning the optimal $q_1, \ldots, q_M$
		\item Straightforward to optimize via coordinate ascent
		\item This is \textbf{NOT} a modeling assumption
	\end{itemize}
	
\end{frame}

\subsection{Coordinate Ascent Variational Inference}
\begin{frame}
	\frametitle{Mean-Field Assumption}
	\begin{itemize}
		\item Interestingly, under the mean-field assumption, the optimization problem for  a single $q_m$ has the solution:
			\begin{equation}
				q_m \left( {\bm Z}_m \right) = \frac{ \exp \left\{ \E_{-m} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\} }{ \int \exp \left\{ \E_{-m} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\} \,d{\bm Z}_m } \quad \forall m
			\end{equation}
		\item This establishes what is called the \textbf{coordinate ascent variational inference} (CAVI) algorithm
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Coordinate Ascent Variational Inference}
	\begin{algorithm}[H]
		\SetKwInput{Input}{Input}
		\SetKwInput{Output}{Output}
		\SetKwInput{Initialize}{Initialize}
		\SetAlgoLined
		\DontPrintSemicolon
		\Input{ Model $p \left( {\bm X}, {\bm Z} \right)$,  Data ${\bm X}$, }
		\Output{Variational density  $q \left( {\bm Z} \right) = \prod_{m=1}^M q_m \left( {\bm Z}_m \right)$}
		\Initialize{Variational densities $q_m \left( {\bm Z}_m \right)$}
		\While{the ELBO has not converged}{
			\For{$m \in \left\{ 1, 2, \ldots, M \right\}$}{
				Set  $q_m \left( {\bm Z}_m \right) \propto \exp \left\{ \E_{-m} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\}$\;
			}
			Compute $\operatorname{ELBO} \left( q \right)$
	  	}
	  	\Return $q \left( {\bm Z} \right)$ \;
	 	\caption{Coordinate ascent variational inference}
	\end{algorithm}
\end{frame}

\subsection{Variational Bayesian EM}
\begin{frame}
	\frametitle{Conjugate-Exponential Models}
	\begin{itemize}
		\item Conjugate-exponential models satisfy two conditions:
			\begin{enumerate}
				\item The complete-data likelihood is in the exponential family, i.e.
					\begin{equation}
						p \left( {\bm X}_i, {\bm Z}_i | {\bm \theta} \right) = g \left( {\bm \theta} \right) f \left( {\bm X}_i, {\bm Z}_i \right) \exp \left\{ \phi \left( {\bm \theta} \right)^{\top} u \left( {\bm X}_i, {\bm Z}_i \right) \right\} \label{ce-1}
					\end{equation}
					where $\phi \left( {\bm \theta} \right)$ is the vector of natural parameters and $u$ is a vector of sufficient statistics
				\item The parameter prior is conjugate to the complete-data likelihood, i.e.
					\begin{equation}
						p \left( {\bm \theta} | \eta, {\bm \nu} \right) = h \left( \eta, {\bm \nu} \right) g \left( {\bm \theta} \right)^{\eta} \exp \left\{ \phi \left( {\bm \theta} \right)^{\top} {\bm \nu} \right\} \label{ce-2}
					\end{equation}
					where $\eta$ and ${\bm \nu}$ are hyperparameters of the prior
			\end{enumerate}
		\item The author in \cite{bealVariationalAlgorithmsApproximate} generalizes the expectation-maximization (EM) to a VB-EM algorithm for conjugate-exponential models (HMMs, MFA, and SSMs)
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian EM}
	\begin{itemize}
		\item Given an i.i.d. dataset ${\bm X} = \left\{ {\bm X}_1, \ldots, {\bm X}_N \right\}$ the $\text{ELBO} \left( q \right)$ can be maximized iteratively
			\begin{enumerate}
				\item The VBE step:
					\begin{equation}
						q \left( {\bm Z}_i \right) \propto f \left( {\bm X}_i, {\bm Z}_i \right) \exp \left\{ \bar{\phi}^{\top} u \left( {\bm X}_i, {\bm Z}_i \right) \right\} \quad \forall i
					\end{equation}
					with 
					\begin{equation*}
						\bar{\phi} = \E_{\bm \theta} \left[ \phi \left( {\bm \theta} \right) \right]
					\end{equation*}
				\item The VBM step:
					\begin{equation}
						q \left( {\bm \theta} \right) = h \left( \tilde{\eta}, \tilde{\bm \nu} \right) g \left( {\bm \theta} \right)^{\tilde{\eta}} \exp \left[ \phi \left( {\bm \theta} \right)^{\top} \tilde{\bm \nu} \right]
					\end{equation}
					with
					\begin{align*}
						\tilde{\eta} &= \eta + N \\
						\tilde{\bm \nu} &= {\bm \nu} + \sum_{i = 1}^N \bar{u} \left( {\bm X}_i \right) \\
						\bar{u} \left( {\bm X}_i \right) &= \E_{{\bm Z}_i} \left[ u \left( {\bm X}_i, {\bm Z}_i \right) \right]
					\end{align*}
			\end{enumerate}
	\end{itemize}
\end{frame}

\begin{comment}
\subsection{Factor Analysis}
\begin{frame}
	\frametitle{VB Framework for Factor Analysis}
	\begin{itemize}
		\item A VB framework for FA models is developed in \cite{nielsen2004variational}
		\item 
			\begin{align}
				p \left( {\bm X}_i | {\bm F}_i, {\bm \Lambda}, {\bm \Sigma} \right) &= \mathcal{N} \left( {\bm X}_i | {\bm \Lambda} {\bm F}_i, {\bm \Sigma} \right) \\
				p \left( {\bm F}_i \right) &= \mathcal{N} \left( {\bm F}_i | {\bf 0}_K, {\bf I}_K \right) \\
				p \left( {\bm \Lambda} | {\bm \alpha} \right) &= \prod_{k=1}^K \mathcal{N} \left( {\bm \lambda}_k | {\bf 0}_N, \frac{1}{\alpha_k} {\bf I}_N \right) \\
				p \left( {\bm \alpha} | {\bm a}^{\alpha}, {\bm b}^{\alpha} \right) &= \prod_{k=1}^K \mathcal{G} \left( \alpha_k | a_k^{\alpha}, b_k^{\alpha} \right) \\
				p \left( {\bm \tau} | {\bm a}^{\tau}, {\bm b}^{\tau} \right) &= \prod_{n=1}^N \mathcal{G} \left( \tau_n | a_n^{\tau}, b_n^{\tau} \right)
			\end{align}
	\end{itemize}
		with the following assumptions:
	\begin{enumerate}
		\item ${\bm F}_t \overset{\text{iid}}{\sim} \mathcal N \left( {\bm 0}_K, \bf{I}_K \right)$
		\item ${\bm e}_t \overset{\text{iid}}{\sim} \mathcal N \left( {\bm 0}_N, \bm \Sigma \right)$ where $\bm \Sigma = \operatorname{diag} \left( \sigma_1^2, \ldots, \sigma_N^2 \right)$
		\item ${\bm F}_t$ and ${\bm e}_s$ are independent for every pair $t, s$
	\end{enumerate}
	where ${\bm 0}_K$ and ${\bm 0}_N$ are zero-vectors of lengths $K$ and $N$, respectively, and ${\bf I}_K$ is the $K \times K$ identity matrix.
\end{frame}

\begin{frame}
	Under the assumptions in the previous slide the likelihood is
	\begin{equation*}
		p \left( {\bf X}_{1:T} | {\bf F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \propto \det \left( {\bf \Sigma} \right)^{-\frac{T}{2}} \exp \left\{ -\frac{1}{2} \sum_{t = 1}^T \left( {\bf X}_t - {\bm \Lambda} {\bf F}_t \right)^{\top} {\bf \Sigma}^{-1} \left( {\bf X}_t - {\bm \Lambda} {\bf F}_t \right) \right\}
	\end{equation*}
	We assign the following prior distributions:
	\begin{align*}
		p \left( {\bm \Lambda} | {\bm \Sigma} \right) &= \prod_{n = 1}^N \mathcal{N} \left( {\bm \lambda}_n \,|\, {\bf 0}_K, \sigma^2_n {\bf I}_K \right) \\
		p \left( {\bm \Sigma} \right) &= \prod_{n = 1}^N \text{Scale-inv-} \chi^2 \left( \sigma^2_n \,|\, \nu_0, \tau^2_0 \right)
	\end{align*}
\end{frame}

\begin{frame}
	The goal here is to find a variational approximation to the posterior distribution over using CAVI, i.e. to find
	\begin{equation*}
		q^{\star} \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \approx p \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} | {\bm X}_{1:T} \right)
	\end{equation*}
	such that 
	\begin{equation*}
		q^{\star} \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) = \argmax_{q \left( {\bm F}, {\bm \Lambda}, {\bm \Sigma}  \right) \in \mathcal Q} \text{ELBO} \left( q \right) \label{new.objective}
	\end{equation*}
	where
	\begin{align*}
		\mathcal Q &= \left\{ q : q \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) = q \left( {\bm F}_{1:T} \right) q \left( {\bm \Lambda} \right) q \left( {\bm \Sigma} \right) \right\} \\
		&= \left\{ q : q \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) = \prod_{t = 1}^T q \left( {\bm F}_t \right) \prod_{n = 1}^N q \left( {\bm \lambda}_n \right) q \left( {\sigma}^2_n \right) \right\}
	\end{align*}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item With the mean-field assumption we have 
			\begin{align*}
				\log q \left( {\bm F}_{1:T} \right) &= \E_{-{\bm F}} \left[ \log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \right] + \text{const} \\
				\log q \left( {\bm \Lambda} \right) &= \E_{-{\bm \Lambda}} \left[ \log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \right] + \text{const} \\
				\log q \left( {\bm \Sigma} \right) &= \E_{-{\bm \Sigma}} \left[ \log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \right] + \text{const}
			\end{align*}
		\item In this case the log-joint $\log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right)$ expands as 
			\begin{equation*}
				\log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm F}_{1:T} \right) + \log p \left( {\bm \Lambda} | {\bm \Sigma} \right) + \log p \left( {\bm \Sigma} \right)
			\end{equation*}
		\item Therefore,
			\begin{align*}
				\log q \left( {\bm F}_{1:T} \right) &= \E_{-{\bm F}} \left[ \log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm F}_{1:T} \right) \right] + \text{const} \\
				\log q \left( {\bm \Lambda} \right) &= \E_{-{\bm \Lambda}} \left[ \log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm \Lambda} | {\bm \Sigma} \right) \right] + \text{const} \\
				\log q \left( {\bm \Sigma} \right) &= \E_{-{\bm \Sigma}} \left[ \log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm \Lambda} | {\bm \Sigma} \right) + \log p \left( {\bm \Sigma} \right) \right] + \text{const}
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Substituting in the expressions for the likelihood and prior for ${\bm F}_{1:T}$ we get
			\begin{equation*}
				\log q \left( {\bm F}_{1:T} \right) = - \frac{1}{2} \sum_{t = 1}^T {\bm F}_t^{\top} \left( {\bf I}_K + \E_{-{\bm F}} \left[ \Lambda^{\top} \bm \Sigma^{-1} \bm \Lambda \right] \right) {\bm F}_t + \sum_{t = 1}^T {\bf X}_t^{\top} \E_{-{\bm F}} \left[ {\bm \Sigma}^{-1} {\bm \Lambda} \right] {\bm F}_t + \text{const}
				\end{equation*}
		\item The equation above is Gaussian wrt ${\bm F}_t$. Thus,
			\begin{equation*}
				q \left( {\bm F}_{1:T} \right) = \prod_{t = 1}^T \mathcal{N} \left( {\bm F}_t \,|\, {\bf m}_{{\bm F} t}, {\bf P}_{\bm F} \right) 
			\end{equation*}
			where
			\begin{align*}
				{\bf P}_{\bm F}^{-1} &= {\bf I}_{K} + \sum_{n = 1}^N \E_{{\bm \Sigma}} \left[ \frac{1}{\sigma^2_n} \right] \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n {\bm \lambda}_n^{\top} \right] \\
				{\bf m}_{{\bm F} t} &= {\bf P}_{\bm F} \, \E_{{\bm \Sigma}} \left[ {\bm \Sigma}^{-1} \right] \E_{{\bm \Lambda}} \left[ {\bm \Lambda} \right] {\bm X}_t
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Proceeding in a similar way, we have that
			\begin{align*}
				q(\bm \Lambda) &= \prod_{n = 1}^N \mathcal{N} \left( {\bm \lambda}_n \,|\, {\bf m}_{{\bm \lambda} n}, {\bf P}_{{\bm \lambda} n} \right) \\
				q({\bm \Sigma}) &= \prod_{n = 1}^N \text{Scale-inv-}\chi^2 \left( \sigma_n^2 \,|\, \nu_{\sigma}, \tau_n^2 \right)
			\end{align*}
			where
			\vspace{-1em}
			\begin{align*}
				{\bf P}_{\bm \lambda n}^{-1} &= \E_{{\bm \Sigma}} \left[ \frac{1}{\sigma_n^2} \right] \left( T \, {\bf I}_{K} + \sum_{t = 1}^T \E_{{\bm F}} \left[ {\bm F}_t {\bm F}_t^{\top} \right] \right) \\
				{\bf m}_{\bm \lambda n} &= {\bf P}_{\bm \lambda n}  \E_{{\bm \Sigma}} \left[ \frac{1}{\sigma_n^2} \right] \sum_{t = 1}^T X_{nt} \E_{{\bm F}} \left[ {\bm F}_t \right] \\
				\nu_{\sigma} &= T + \nu_0\\
				\nu_{\sigma} \tau_n^2 &= \nu_0 \tau_0^2 + \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n^{\top} {\bm \lambda}_n \right] + \sum_{t = 1}^T \left[ X_{nt}^2 - 2 X_{nt} \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n \right]^{\top} \E_{{\bm F}} \left[ {\bm F}_t \right] + \tr \left[ \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n {\bm \lambda}_n^{\top} \right] \E_{{\bm F}} \left[ {\bm F}_t {\bm F}_t^{\top} \right] \right] \right]
			\end{align*}
	\end{itemize}
\end{frame}
\end{comment}

\section{Future Work}
\begin{frame}
	\frametitle{Future Work}
	\begin{itemize}
		\item Ultimate goal is to develop a VB framework to handle the three- and four-level hierarchical dynamic factor model in \cite{moenchDynamicHierarchicalFactor2013}		
		\item Developing a VB framework according to CAVI is straightforward, but very tedious
			\begin{itemize}
				\item[--] Still an open problem
			\end{itemize}
		\item Developing a VB framework according to the VBEM using the degenerate state space representation is not as straightforward
			\begin{itemize}
				\item[--] The state transition covariance matrix is singular
				\item[--] The state transition equations have time-varying intercepts
				\item[--] Still an open problem
			\end{itemize}
	\end{itemize}
\end{frame}

\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{Selected References}
	\printbibliography
\end{frame}

\end{document}